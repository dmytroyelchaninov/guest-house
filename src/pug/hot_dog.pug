extends base.pug

//- This adds extra styles and js to already defined in base.pug
append main_vars
  - css_main.push.apply(css_main, ["css/hot_dog.css", "css/description_hot.css"])
  - js_main.push.apply(js_main, ["js/hot_dog.js"])
  
block content
    .hot__wrapper
        .hot__description
            .hot__description_title
                h1#hot__title What's that? <br>
            .hot__description_text
                p#hot__text
                    p#drop__title UPLOAD THE IMAGE TO CHECK IF IT'S A HOT DOG! ðŸŒ­ <br>
                p#hot__text
                    | My app uses a state-of-the-art Vision Transformer (ViT) model, which is more powerful than traditional Convolutional Neural Networks (CNNs) 
                    | like VGG or MobileNet. ViT analyzes the entire image at once, picking up global patterns and details that CNNs often miss. <br>
                    | Thanks to its global attention mechanism, ViT hits a higher accuracy, outperforming CNNs in most cases. <br>
                    | Can your picture break my model? <br><br>
                    | By the way, welcome to my
                    a#git-link(href="https://github.com/dmytroyelchaninov/is_hot_dog")  Github<br>
                    | and scroll down for more details!
            .hot__description_arrow
                buttom#hot__arrow &#10094;
        .hot__images
            .hot__images_dragdrop
                img#hot__dragdrop(src="images/dragdrop.png", alt="Drag and Drop")
            .hot__images_hotdog
                img#hot__hotdog(src="images/hot_title.png", alt="Hot Dog")
                form#upload-form(method="post", enctype="multipart/form-data")
                input#file-input(type="file", name="file", style="display:none;", accept="image/*")
            .hot__images_uploaded-image
                img#hot__uploaded-image(src="", alt="Uploaded Image")
                form#upload-form(method="post", enctype="multipart/form-data")
                input#file-input(type="file", name="file", style="display:none;", accept="image/*")
        .hot__result
            //- .hot__result_processing
            //-     span#rotating-dash 
            //-     p#processing_text Processing...
            .hot__result_text
                p#hot__result-pos
                    | Nice hot dog! ðŸŒ­
                p#hot__result-neg
                    | That's definetely not a hot dog! ðŸš«
        .hot__notification-process
            p Hold on, image is processing
        .hot__notification-error
            p Oops, something went wrong

    .desc__describe_container
        .desc__describe_wrapper
            .desc__describe_title
                h1#desc__describe_title How does it work?
            
            .desc__describe_main
                .desc__describe_main_text
                    p When thinking about how large language models (LLMs) work, I came up with an idea. LLMs learn on sequences of words, which are split into tokens. The transformer architecture in LLMs allows the model to learn large-scale dependencies across those tokens, no matter their position in the sequence. This is quite different from how CNNs (Convolutional Neural Networks) work, as CNNs focus on extracting local features (edges, textures) using convolutional filters that process small regions of the image at a time.
                    p But what if we used transformers for images? This could allow us to capture more general patterns beyond local features, and potentially achieve higher accuracy, especially in image classification problems. Of course, using transformers for images would require more memory and computational resources. However, if our goal is accuracy rather than real-time predictions, this might be worth it.
                    p Initially, I considered flattening the entire image into a sequence. But the problem with that is the number of tokens would be too large. For example, with a 256x256 RGB image (with 3 color channels), flattening it would result in 256 * 256 * 3 = 196,608 tokens. Transformers have a time complexity of O(nÂ²), where n is the sequence length. This means processing such long sequences would be extremely computationally expensive. A more practical approach would be to split the image into smaller patches (for example, 16x16 pixels) and treat each patch as a token.
                    p And, of course, almost any interesting idea you think of has already been implemented in some form. Enter the Vision Transformer (ViT), which leverages the same principles I just mentioned.
                    
                    .desc__vision_transformer_section
                        h2 Vision Transformer (ViT): How It Works
                        ol#desc__vit_steps
                            li <b>Image Patches</b>. Instead of processing each pixel individually, ViT divides the image into fixed-size patches (e.g., 16x16 pixels). Each patch is treated as a token.
                            li <b>Tokenization</b>. The patches are flattened and treated as input tokens for the transformer.
                            li <b>Linear Projection</b>. Each patch is linearly projected into a fixed-dimensional embedding space.
                            li <b>Positional Encoding</b>. Since transformers donâ€™t have a natural sense of the position of tokens (unlike CNNs that process images spatially), ViT adds positional encodings to the patch embeddings. This allows the model to learn where each patch belongs in the image.
                            li </b>Transformer Layers</b>. These patch embeddings with positional encodings are passed through a series of transformer layers, where self-attention captures global dependencies across all patches.
                            li <b>Classification Head</b>. A special classification token is appended to the input sequence, and its final output is used for the classification decision.

                    .desc__application_section
                        h2 Application: Hotdog Classification
                        p I recently applied this idea to a simple binary classification taskâ€”whether an image contains a hotdog or not. I participated in a short hackathon where the goal was to build the most accurate model for this task. The dataset consisted of small (32x32x3) images, and the problem was surprisingly difficultâ€”some images were so unclear that even I found it hard to tell if they contained a hotdog.
                        p Using CNN-based models (including pre-trained models), I was able to achieve around 83-85% accuracy on the test data. Even with high-end GPUs, that was the best I could do. This was a perfect example of a task with a simple goal but complex data.
                        p When I switched to ViT, the model immediately surpassed 90% accuracy on the validation data. After about one hour of training, it achieved 97% accuracy on the test data, and I stopped the process even though the validation loss was still decreasing!

                    .desc__summary_section
                        h2 Summary
                        p Models based on transformers, like ViT, are absolutely viable for image classification tasks, but they come with trade-offs. They require more computational resources, both for training and inference, compared to CNNs. However, if high accuracy is your priority, especially for classification tasks, using a pre-trained ViT model can be a highly effective approach. The key is to balance the computational cost against the accuracy requirements of your project.

                .desc__describe_main_img
                    p#desc__describe_img_text Image generated by DALLÂ·E based on the description above.
                    img#desc__describe_img(src="images/hot_desc.jpg", alt="Vision Transformer")