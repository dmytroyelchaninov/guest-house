extends base.pug

append main_vars
    - css_main.push.apply(css_main, ["css/game.css", "css/description_darts.css"])
    - js_main.push.apply(js_main, ["js/game.js"])

block content
    main
        .game-container
            .explanation
                .explanation__title
                    h1#explanation__title What's going on here?! ðŸŽ¯
                .explanation__text
                    p#explanation__description 
                        | Upload a picture of your dartboard, and the app will automatically detect and 
                        | track dart positions using advanced image processing with OpenCV, YOLO and clustering methods. <br>
                        | If you click on the TEST button, a random image from the test library will be uploaded. <br>
                        | SWIPE RIGHT TO SEE UNPROCESSED IMAGE <br>
                        | May be little slow, due to low-end the CPU on the server and iterative clustering, <br>
                        | but soon it will become much faster. (locally takes around 2sec on regular computer) <br><br>
                        | You can check the full app on my 
                        a#git-link(href="https://github.com/dmytroyelchaninov/dart_game_ai") Github. <br>
                        | Additional feautures like automatic Airdrop for Apple users, game logic and other functionalities are there.<br><br>
                        | Scroll down for more details!
                .explanation__arrow
                    buttom#explanation__arrow &#10094;
            .crowns
                img#crowns(src="images/crowns.jpg", alt="Crowns")

            #image-wrapper.image-wrapper
                img#dartboard.game-image(src="images/title.png", alt="Dartboard")
                form#upload-form(method="post", enctype="multipart/form-data")
                input#file-input(type="file", name="file", style="display:none;", accept="image/*")
        
            // Carousel container (hidden initially)
            #carousel.carousel(style="display: none;")
                .carousel-images
                    img#carousel-processed.carousel-image(src="", alt="Processed Image")
                    img#carousel-initial.carousel-image(src="", alt="Initial Image")
                    form#upload-form-carousel(method="post", enctype="multipart/form-data")
                    input#file-input-carousel(type="file", name="file", style="display:none;", accept="image/*")
            
            .test-button
                img#test-button(src="images/test_me.jpg", alt="Test Me", onclick="testImage()")

            // Move the carousel control buttons outside the carousel container
            button.carousel-control.prev(onclick="prevSlide()") &#10094;
            button.carousel-control.next(onclick="nextSlide()") &#10095;

        .notification-process
            p Hold on, image is processing
        .notification-error
            p Oops, something went wrong
    
    //- .desc__container
    //-     .desc__wrapper
    //-         .desc__title
    //-             h1#desc__title How does it work?
            
    //-         .desc__main
    //-             .desc__main_text
    //-                 p#desc__header 
    //-                 | This app is designed to automate the process of detecting dart hits on a dartboard and accurately calculating the score, 
    //-                 | overcoming the challenges of perspective distortion and varied angles. <br>
    //-                 | Hereâ€™s a breakdown of the core functionality:
                    
    //-                 .desc__step1
    //-                     h2 1. Initial Dart Detection with YOLO Models
    //-                     p The first step in the process involves detecting the number of darts on the board and their approximate locations:
    //-                     ol#desc__step1
    //-                         li <b>Image Patches</b>. Instead of processing each pixel individually, ViT divides the image into fixed-size patches (e.g., 16x16 pixels). Each patch is treated as a token.
    //-                         li <b>Tokenization</b>. The patches are flattened and treated as input tokens for the transformer.
    //-                         li <b>Linear Projection</b>. Each patch is linearly projected into a fixed-dimensional embedding space.
    //-                         li <b>Positional Encoding</b>. Since transformers donâ€™t have a natural sense of the position of tokens (unlike CNNs that process images spatially), ViT adds positional encodings to the patch embeddings. This allows the model to learn where each patch belongs in the image.
    //-                         li </b>Transformer Layers</b>. These patch embeddings with positional encodings are passed through a series of transformer layers, where self-attention captures global dependencies across all patches.
    //-                         li <b>Classification Head</b>. A special classification token is appended to the input sequence, and its final output is used for the classification decision.

    //-                 .desc__step2
    //-                     h2 1. Initial Dart Detection with YOLO Models
    //-                     p The first step in the process involves detecting the number of darts on the board and their approximate locations:
    //-                     ol#desc__step2
    //-                         li <b>Image Patches</b>. Instead of processing each pixel individually, ViT divides the image into fixed-size patches (e.g., 16x16 pixels). Each patch is treated as a token.
    //-                         li <b>Tokenization</b>. The patches are flattened and treated as input tokens for the transformer.
    //-                         li <b>Linear Projection</b>. Each patch is linearly projected into a fixed-dimensional embedding space.
    //-                         li <b>Positional Encoding</b>. Since transformers donâ€™t have a natural sense of the position of tokens (unlike CNNs that process images spatially), ViT adds positional encodings to the patch embeddings. This allows the model to learn where each patch belongs in the image.
    //-                         li </b>Transformer Layers</b>. These patch embeddings with positional encodings are passed through a series of transformer layers, where self-attention captures global dependencies across all patches.
    //-                         li <b>Classification Head</b>. A special classification token is appended to the input sequence, and its final output is used for the classification decision.

    //-                 .desc__summary
    //-                     h2 Summary
    //-                     p Models based on transformers, like ViT, are absolutely viable for image classification tasks, but they come with trade-offs. They require more computational resources, both for training and inference, compared to CNNs. However, if high accuracy is your priority, especially for classification tasks, using a pre-trained ViT model can be a highly effective approach. The key is to balance the computational cost against the accuracy requirements of your project.

    //-                 .desc__notes
    //-                     h2 Some Notes:
    //-                     ol
    //-                         li <b>Linear Projection</b>. This step transforms each image patch into a fixed-dimensional embedding (similar to how words are embedded in NLP tasks). It doesnâ€™t directly map onto axes like mathematical projections but is a learned transformation through a weight matrix.
    //-                         li <b>Positional Encodings</b>. These help transformers handle spatial data like images, as they ensure the model understands where each patch is located in the original image.
    //-                         li <b>ViTâ€™s Advantage</b>. The use of self-attention allows ViT to capture global patterns in an image better than CNNs, which often focus on local features. However, transformersâ€™ quadratic complexity means they are more resource-intensive.
                        
    //-             .desc__img
    //-                 p#desc__img_text Image generated by DALLÂ·E based on the description above.
    //-                 img#desc__img(src="images/hot_desc.jpg", alt="Vision Transformer")
